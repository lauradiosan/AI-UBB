{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probleme de clasificare nesupervizata (clustering) <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/Cluster-2.svg\" width=\"150\">\n",
    "\n",
    "## Obiective\n",
    "* rezolvarea unei probleme de clustering (metoda bazata pe k-means)\n",
    "* extragerea de caracteristici din informatiile textuale\n",
    "\n",
    "## Cuvinte cheie:\n",
    "* date de antrenare si date de testare \n",
    "* atribute/catacteristici ale datelor\n",
    "* etichete ale datelor\n",
    "* normalizare date \n",
    "* model de clusterizare\n",
    "* acuratetea clusterizarii\n",
    "\n",
    "\n",
    "## Aspecte teoretice\n",
    "\n",
    "Clasificarea nesupervizata, numita si clusterizare, are drept scop invatarea unei modalitati de grupare a datelor pentru care nu au fost oferite in prealabil detalii despre apartententa lor la anumite clase. In loc sa raspunda la un feedback (construit pe baza etichetelor asociate exemplelor de antrenament), clasificarea nesuepervizata identifica asemanari intre date. \n",
    "\n",
    "Se doreste impartirea unor exemple neetichetate în submultimi disjuncte (clusteri) astfel incat:\n",
    "* exemplele din acelasi cluster sunt foarte similare\n",
    "* exemplele din clusteri diferiti sunt foarte diferite\n",
    "\n",
    "\n",
    "### Exemple de probleme de clusterizare:\n",
    "* segmentarea clientilor de pe o anumita piata  \n",
    "* analize in retele sociale (identificarea unor grupuri cu anumite particularitati)\n",
    "* compresia imaginilor\n",
    "\n",
    "### Formalizare problema de clusterizare:\n",
    "\n",
    "Clusterizarea presupune identificarea unui model de asociere exemplu-eticheta.\n",
    "Se dau:\n",
    "* un set de date de train si un set de date de test\n",
    "* fiecare data (exemplu) se caracterizeaza prin $m$ $atribute$ ($x = (x_1, x_2, ..., x_m)$) (atributele pot fi valori numerice sau valori nenumerice)\n",
    "\n",
    "Se cere sa se determine o functie (necunoscuta) care realizeaza gruparea datelor de antrenament in mai multe clase\n",
    "* nr de clase poate fi pre-definit (k) sau necunoscut\n",
    "* datele dintr-o clasa sunt asemanatoare\n",
    "* clasa asociata unei date (noi) de test folosind gruparea invatata pe datele de antrenament\n",
    "\n",
    "\n",
    "\n",
    "### Algoritmi\n",
    "* k-means <img src=\"http://shabal.in/visuals/kmeans/random.gif\" width=\"150\">\n",
    "\n",
    "\n",
    "Pasi:\n",
    "\n",
    "1. Se initializeaza k centroizi $\\mu_1$, $\\mu_2$, ..., $\\mu_{k}$. Un centroid $\\mu_j$ este un vector cu $m$ valori ($m$ - nr de atribute). Initializarea se poate realiza:\n",
    "* pur random\n",
    "* alegand, random, $k$ exemple (din setul de date de antrenament)\n",
    "\n",
    "2. Se repeta pana la convergenta: \n",
    "\n",
    "- Fiecare exemplu $x^i$ din setul de antrenament se asociaza celui mai apropiat centroid (i se asociaza eticheta $c^i$)\n",
    "$$c^i = \\arg\\min_{j=1,2,  \\ldots, k}{dist(x^i, \\mu_j)},$$\n",
    "\n",
    "unde $dist(a,b)$ este o functie care masoara distanta intre 2 exemple $a$ si $b$ (intre atributele lor).\n",
    "\n",
    "- Se recalculeaza centroizii prin mutarea lor in media exemplelor asociate fiecaruia\n",
    "\n",
    "$$ \\mu_j = \\frac{\\sum_{i=1,2, \\ldots, N}{1_{c^i=j} x^i}}{\\sum_{i=1,2, \\ldots, N}{1_{c^i=j}}}$$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerinte \n",
    "\n",
    "Specificaţi, proiectaţi şi implementaţi rutine de rezolvare a unei probleme de reclusterizare folosind metoda k-means\n",
    "\n",
    "**Problema live: Clusterizare mesaje - se doreste clusterizarea unor mesaje in doua categorii (spam si ham). Pentru fiecare mesaj se cunoaste textul aferent lui.**\n",
    "\n",
    "\n",
    "**Problema tema: Clasificarea de texte pe baza sentimentelor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema live: Clusterizare mesaje - se doreste clusterizarea unor mesaje in doua categorii (spam si ham). Pentru fiecare mesaj se cunoaste textul aferent lui.**\n",
    "\n",
    "Proces:\n",
    "* Se pleaca de la un set de date format din textul mesajelor precum cel din fisierul spam csv [link](data/spam.csv).\n",
    "* Se imparte setul de date in date de antrenament si in date de test\n",
    "* Se extrag anumite caracteristici din textul mesajelor folosind diferite reprezentari precum:\n",
    "    - Bag of Words\n",
    "    - TF-IDF\n",
    "    - Word2Vec\n",
    "* Se aplica algoritmul k-means pe setul de antrenament si se identifica cei doi centroizi (corespunzatori clusterilor spam si ham, respectiv)\n",
    "* Fiecare mesaj din setul de test se asociaza acelui cluster pentru care distanta dintre caracteristicile mesajului si centroid este minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "crtDir =  os.getcwd()\n",
    "fileName = os.path.join(crtDir, 'data', 'spam.csv')\n",
    "\n",
    "data = []\n",
    "with open(fileName) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            dataNames = row\n",
    "        else:\n",
    "            data.append(row)\n",
    "        line_count += 1\n",
    "\n",
    "inputs = [data[i][0] for i in range(len(data))]\n",
    "outputs = [data[i][1] for i in range(len(data))]\n",
    "labelNames = list(set(outputs))\n",
    "\n",
    "print(inputs[:2])\n",
    "print(labelNames[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Probably, want to pick up more?', \"No go. No openings for that room 'til after thanksgiving without an upcharge.\", \"Fuck babe ... I miss you already, you know ? Can't you let me send you some money towards your net ? I need you ... I want you ... I crave you ...\"]\n"
    }
   ],
   "source": [
    "# prepare data for training and testing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "# noSamples = inputs.shape[0]\n",
    "noSamples = len(inputs)\n",
    "indexes = [i for i in range(noSamples)]\n",
    "trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace = False)\n",
    "testSample = [i for i in indexes  if not i in trainSample]\n",
    "\n",
    "trainInputs = [inputs[i] for i in trainSample]\n",
    "trainOutputs = [outputs[i] for i in trainSample]\n",
    "testInputs = [inputs[i] for i in testSample]\n",
    "testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "print(trainInputs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "vocab:  ['00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '0125698789', '02']\nfeatures:  [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
    }
   ],
   "source": [
    "# extract some features from the raw text\n",
    "\n",
    "# # representation 1: Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "testFeatures = vectorizer.transform(testInputs)\n",
    "\n",
    "# vocabbulary from the train data \n",
    "print('vocab: ', vectorizer.get_feature_names()[:10])\n",
    "# extracted features\n",
    "print('features: ', trainFeatures.toarray()[:3][:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "vocab:  ['all', 'and', 'are', 'at', 'be', 'but', 'call', 'can', 'do', 'for']\nfeatures:  [[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.48953734 0.87198234\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.32963209 0.         0.\n  0.         0.41437418 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.77258758 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.35035005 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.20529303 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.23693671\n  0.         0.         0.17904984 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.91380454 0.18605964]]\n"
    }
   ],
   "source": [
    "# representation 2: tf-idf features - word granularity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "\n",
    "trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "testFeatures = vectorizer.transform(testInputs)\n",
    "\n",
    "# vocabbulary from the train data \n",
    "print('vocab: ', vectorizer.get_feature_names()[:10])\n",
    "# extracted features\n",
    "print('features: ', trainFeatures.toarray()[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('supporting', 0.6251285076141357), ('suport', 0.6071150302886963), ('suppport', 0.6053199768066406), ('Support', 0.6044273376464844), ('supported', 0.6009396910667419), ('backing', 0.6007589101791382), ('supports', 0.5269277095794678), ('assistance', 0.5207138061523438), ('sup_port', 0.5192490220069885), ('supportive', 0.5110025405883789)]\nvec for house:  [ 1.57226562e-01 -7.08007812e-02  5.39550781e-02 -1.89208984e-02\n  9.17968750e-02  2.55126953e-02  7.37304688e-02 -5.68847656e-02\n  1.79687500e-01  9.27734375e-02  9.03320312e-02 -4.12109375e-01\n -8.30078125e-02 -1.45507812e-01 -2.37304688e-01 -3.68652344e-02\n  8.74023438e-02 -2.77099609e-02  1.13677979e-03  8.30078125e-02\n  3.57421875e-01 -2.61718750e-01  7.47070312e-02 -8.10546875e-02\n -2.35595703e-02 -1.61132812e-01 -4.78515625e-02  1.85546875e-01\n -3.97949219e-02 -1.58203125e-01 -4.37011719e-02 -1.11328125e-01\n -1.05957031e-01  9.86328125e-02 -8.34960938e-02 -1.27929688e-01\n -1.39648438e-01 -1.86523438e-01 -5.71289062e-02 -1.17675781e-01\n -1.32812500e-01  1.55639648e-02  1.34765625e-01  8.39843750e-02\n -9.03320312e-02 -4.12597656e-02 -2.51953125e-01 -2.27539062e-01\n -6.64062500e-02 -7.66601562e-02  5.15136719e-02  5.90820312e-02\n  3.49609375e-01 -1.13769531e-01 -2.57568359e-02 -1.98242188e-01\n  4.44335938e-02  1.09863281e-01  1.04003906e-01 -1.75781250e-01\n  1.22558594e-01  7.81250000e-02  6.20117188e-02  6.49414062e-02\n -1.73828125e-01 -1.11694336e-02  1.88476562e-01  3.34472656e-02\n -4.29687500e-02 -4.71191406e-02  2.91015625e-01  4.19921875e-02\n  1.59179688e-01  1.22558594e-01 -2.55859375e-01  2.44140625e-01\n  1.54296875e-01 -3.46679688e-02  1.24023438e-01 -1.32812500e-01\n  8.44726562e-02  3.71093750e-02 -1.05468750e-01  9.81445312e-02\n -8.23974609e-03  5.34667969e-02 -1.96838379e-03  9.03320312e-02\n  1.30859375e-01 -1.57470703e-02 -2.40478516e-02 -3.29589844e-02\n -5.63964844e-02 -3.12500000e-01 -1.19140625e-01  4.41894531e-02\n -1.82617188e-01 -2.20703125e-01  8.39843750e-02 -2.15820312e-01\n -1.60156250e-01 -2.01171875e-01  1.63085938e-01 -4.57763672e-05\n  4.24804688e-02 -1.37695312e-01 -2.62451172e-02  1.53320312e-01\n -1.07421875e-01 -1.34765625e-01 -3.73840332e-03 -1.51977539e-02\n -7.27539062e-02  3.22265625e-02  1.89453125e-01 -8.00781250e-02\n  1.45507812e-01 -9.66796875e-02 -9.27734375e-02  8.91113281e-03\n -4.27246094e-02 -9.76562500e-02  3.29589844e-02 -7.95898438e-02\n -6.25000000e-02  3.39355469e-02  1.05590820e-02 -1.28906250e-01\n  1.09863281e-01  1.89453125e-01  1.52343750e-01 -1.47460938e-01\n -3.86047363e-03  1.75781250e-01 -4.58984375e-02 -1.02539062e-01\n  6.34765625e-02 -9.86328125e-02  1.87500000e-01  3.97949219e-02\n -2.65625000e-01 -1.24023438e-01 -1.35742188e-01  7.93457031e-03\n  6.59179688e-02  8.11767578e-03 -3.24707031e-02 -1.03759766e-02\n -1.90429688e-02 -8.20312500e-02  2.06054688e-01  1.40625000e-01\n  1.93359375e-01  2.91015625e-01 -9.17968750e-02 -1.40625000e-01\n -1.75781250e-01 -1.36718750e-01  2.51464844e-02 -5.83496094e-02\n -1.84570312e-01  3.10546875e-01  7.17773438e-02 -1.01074219e-01\n  1.08886719e-01 -2.23388672e-02  1.50390625e-01 -7.03125000e-02\n  1.24023438e-01  2.21679688e-01 -1.97265625e-01 -6.05468750e-02\n -4.30297852e-03 -1.69921875e-01 -1.45507812e-01 -2.17773438e-01\n  2.47070312e-01  6.64062500e-02 -8.05664062e-02  3.57421875e-01\n -8.20312500e-02 -7.87353516e-03  1.08886719e-01 -5.32226562e-02\n  3.00781250e-01 -2.37304688e-01  1.61132812e-01  1.59179688e-01\n  1.69921875e-01 -9.52148438e-02  5.20019531e-02 -6.22558594e-02\n -8.85009766e-03  4.68750000e-02 -2.88085938e-02  1.25000000e-01\n  3.49121094e-02  4.61425781e-02  1.66015625e-02 -9.57031250e-02\n -1.48437500e-01 -1.64794922e-02 -2.22656250e-01 -2.51953125e-01\n -3.58886719e-02 -2.52685547e-02  8.39233398e-05  6.98242188e-02\n  2.53906250e-01 -3.29589844e-02  6.59179688e-02  6.28662109e-03\n -7.86132812e-02 -3.01513672e-02 -9.47265625e-02  1.25000000e-01\n -1.62109375e-01  2.53906250e-01 -3.30078125e-01  6.44531250e-02\n -9.09423828e-03  7.12890625e-02  3.99780273e-03 -4.41894531e-02\n -1.42822266e-02 -9.52148438e-03  1.17675781e-01  3.49609375e-01\n -2.90527344e-02  1.86767578e-02  3.46679688e-02  1.89208984e-02\n -1.26953125e-01  2.68554688e-02 -1.06933594e-01  1.20117188e-01\n -2.69775391e-02 -5.07812500e-02 -1.76757812e-01  3.95507812e-02\n  1.35742188e-01 -9.61914062e-02 -1.98242188e-01 -1.86767578e-02\n -2.47802734e-02 -5.32226562e-02  1.54296875e-01  5.95703125e-02\n  6.39648438e-02 -6.17675781e-02  3.36914062e-02  1.75781250e-01\n  6.59179688e-02  2.22656250e-01 -1.28906250e-01  4.61425781e-02\n -2.57812500e-01  6.78710938e-02  6.29882812e-02 -1.15722656e-01\n -2.13867188e-01 -2.53906250e-01  2.73437500e-02 -4.68750000e-02\n  1.38671875e-01  2.59765625e-01 -2.07031250e-01 -9.64355469e-03\n -5.22460938e-02 -7.20214844e-03  8.49609375e-02 -2.49023438e-02\n  1.94335938e-01 -7.37304688e-02  1.22070312e-01 -3.49121094e-02\n  1.41601562e-01 -1.38671875e-01  7.61718750e-02 -1.93359375e-01\n  1.64062500e-01 -2.78320312e-02 -1.45263672e-02  1.44531250e-01\n  1.75781250e-01 -1.70898438e-02  1.26953125e-01  3.39355469e-02\n -2.80761719e-02  1.82617188e-01 -2.94921875e-01  3.78417969e-02\n -1.63085938e-01  1.73828125e-01 -1.01074219e-01 -1.49414062e-01\n -4.17480469e-02  9.82666016e-03 -4.94384766e-03 -3.29589844e-02]\n"
    }
   ],
   "source": [
    "# representation 3: embedded features extracted by a pre-train model (in fact, word2vec pretrained model)\n",
    "\n",
    "import gensim \n",
    "\n",
    "# Load Google's pre-trained Word2Vec \n",
    "crtDir =  os.getcwd()\n",
    "modelPath = os.path.join(crtDir, 'models', 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "word2vecModel300 = gensim.models.KeyedVectors.load_word2vec_format(modelPath, binary=True) \n",
    "print(word2vecModel300.most_similar('support'))\n",
    "print(\"vec for house: \", word2vecModel300[\"house\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def featureComputation(model, data):\n",
    "    features = []\n",
    "    phrases = [ phrase.split() for phrase in data]\n",
    "    for phrase in phrases:\n",
    "        # compute the embeddings of all the words from a phrase (words of more than 2 characters) known by the model\n",
    "        vectors = [model[word] for word in phrase if (len(word) > 2) and (word in model.vocab.keys())]\n",
    "        if len(vectors) == 0:\n",
    "            result = [0.0] * model.vector_size\n",
    "        else:\n",
    "            result = np.sum(vectors, axis=0) / len(vectors)\n",
    "        features.append(result)\n",
    "    return features\n",
    "\n",
    "trainFeatures = featureComputation(word2vecModel300, trainInputs)\n",
    "testFeatures = featureComputation(word2vecModel300, testInputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "acc:  0.8582959641255605\n"
    }
   ],
   "source": [
    "# unsupervised classification ( = clustering) of data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "unsupervisedClassifier = KMeans(n_clusters=2, random_state=0)\n",
    "unsupervisedClassifier.fit(trainFeatures)\n",
    "computedTestIndexes = unsupervisedClassifier.predict(testFeatures)\n",
    "computedTestOutputs = [labelNames[value] for value in computedTestIndexes]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"acc: \", accuracy_score(testOutputs, computedTestOutputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema tema: Clasificarea textelor pe baza sentimentelor**\n",
    "\n",
    "Mai tii minte ca tocmai ti-ai inceput prima ta zi de munca ca si software developer la Facebook si ca faci parte din echipa care se ocupa cu partea de continut a platformei? \n",
    "\n",
    "Utilizatori sunt foarte incantati de noul algoritm de detectie a filtrelor si emotilor in poze, asadar poti sa te ocupi de o noua functionalitate care ar face platforma mai atractiva. Utilizatorii posteaza o gama larga de mesaje, iar in feed-urile lor apar de multe ori prea multe mesaje negative si prea putine pozitive. Facebook incearca o noua functionalitate prin care sa detecteze sentimentele dintr-un mesaj si sa filtreze feed-urile utilizatorilor.\n",
    "\n",
    "Task-ul tau este sa implementez un algoritm care poate recunoaste sentimentele dintr-un text (pozitiv, negativ, ura, rasism, etc.)\n",
    "\n",
    "\n",
    "Team leaderul echipei de ML iti propune urmatorul plan de lucru \n",
    "\n",
    "1.\tdevoltarea, antrenarea si testarea unui algoritm de tip k-means folosind data de tip numeric (de ex datele cu irisi) \n",
    "\n",
    "2.\tConsiderarea unei baze cu texte etichetate cu emotii (de ex. textele din data/review_mixed.csv sau https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets)\n",
    "\n",
    "3.\tExtragerea de caracteristici din texte folosind diferite reprezentari precum:\n",
    "    - Bag of Words\n",
    "    - TF-IDF\n",
    "    - Word2Vec\n",
    "    - Bag of N-grams, etc.\n",
    "\n",
    "4.\tClasificarea textelor si etichetarea lor cu emotii folosind\n",
    "    - un algoritm de invatare supervizat (folosind etichetele pt emotiile asociate fiecarui text)\n",
    "    - un algoritm de invatare nesupervizat bazat pe k-means (fara a folosi etichetele pt emotiile asociate fiecarui text)\n",
    "    - un algoritm hibrid care combina tehncile de invare cu reguli ajutatoare (de ex prin folosirea unor reguli care verifica/numara aparitiile unor cuvinte - polarized words - (e.g. negative words such as bad, worst, ugly, etc and positive words such as good, best, beautiful, etc.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitmyenvvenv9d2ef12a62c64e369825be189b104135",
   "display_name": "Python 3.7.2 64-bit ('myenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}